{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fVmsLZpuMP5"
      },
      "source": [
        "### ***Link To drive***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6grUi1j_W1sv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XCPyA6uuYUc"
      },
      "source": [
        "### ***Import Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvMGkMPwyieZ"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw-NugY2ZstK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lz63TAcufbN"
      },
      "source": [
        "### ***Load Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpNLiYu4uemU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyLE0nW7ujo4"
      },
      "source": [
        "### ***Preprocessing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7quBrG1FwTBk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import nltk # Make sure to import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess text with lemmatization and stemming\n",
        "def preprocess_text_full(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove special characters, digits, and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in lemmatized]\n",
        "    # Join tokens back into string\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "# Apply full preprocessing to the 'text' column\n",
        "df['full_preprocessed_text'] = df['posts'].apply(preprocess_text_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIlbVIY_XnjY"
      },
      "outputs": [],
      "source": [
        "posts_split = df['posts'].str.split('\\|\\|\\|')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJkkDKynBkY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoVQ_N74zDDJ"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data for  traits\n",
        "df['NS'] = df['posts'].apply(lambda x: x[1])\n",
        "df['class'] = df['posts'].apply(lambda x: 0 if x.startswith('N') else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRQEQ0XJhHoX"
      },
      "source": [
        "### ***Machine Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lJtwv-unur"
      },
      "source": [
        "### ***Apply POS TAGGING***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8xbX3cpzQFm"
      },
      "outputs": [],
      "source": [
        "# Load the model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return \" \".join(pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2UDT_QXxj6l"
      },
      "outputs": [],
      "source": [
        "# Apply POS tagging to the posts\n",
        "df['pos_tags'] = df['posts'].apply(pos_tagging)\n",
        "\n",
        "X_ns = df['pos_tags']\n",
        "y_ns = df['NS']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction for all four traits\n",
        "# Step 2: Feature Extraction\n",
        "posts_combined = posts_split.apply(lambda x: ' '.join(x))\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_ns = tfidf_vectorizer.fit_transform(posts_combined)"
      ],
      "metadata": {
        "id": "zEK0VYIA6Es6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Vhl5OYX7A38G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "sH64bao9BAQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7KVCkc9_8bf"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(X_ns, y_ns, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_ns = le.fit_transform(y_train_ns)\n",
        "y_test_ns = le.transform(y_test_ns)"
      ],
      "metadata": {
        "id": "hD6axUhqBQda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}"
      ],
      "metadata": {
        "id": "nBX_0M6fBOa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XldsyaG8_fzQ"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDnx0I8xI7Xh"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_model(X_train_ns, X_test_ns, y_train_ns, y_test_ns, \"Intuition vs. Sensing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTfQjTul-ktJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_vectorized, y_train_ns)\n",
        "\n",
        "    # Get scores (decision function output) on the test set\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_test_vectorized)\n",
        "    else:\n",
        "        scores = model.predict(X_test_vectorized)\n",
        "\n",
        "    # Convert scores into probabilities\n",
        "    y_pred_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Compute ROC curve and ROC area for Introversion vs. Extroversion (IE) trait\n",
        "    fpr, tpr, _ = roc_curve(y_test_ns, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the model\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Calculate mean true positive rate\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "    # Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve for N/S POS tagging')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize Naive Bayes classifier\n",
        "model = MultinomialNB()"
      ],
      "metadata": {
        "id": "87B723r_5pL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_ns = accuracy_score(y_test_ns, y_pred_ns)\n",
        "\n",
        "print(\"\\nIntuition vs. Sensing:\")\n",
        "print(f\"Accuracy: {accuracy_ns}\")\n",
        "print(classification_report(y_test_ns, y_pred_ns))"
      ],
      "metadata": {
        "id": "YMUTChqC5yyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")"
      ],
      "metadata": {
        "id": "fKgrvkFD5loX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8b-LkwK5m1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVHw93fiutyF"
      },
      "source": [
        "### ***TF-IDF***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a893c57"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL0tT7VxXh8p"
      },
      "outputs": [],
      "source": [
        "# Feature extraction for all four traits\n",
        "# Step 2: Feature Extraction\n",
        "posts_combined = posts_split.apply(lambda x: ' '.join(x))\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_ns = tfidf_vectorizer.fit_transform(posts_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3eUSA88aotz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_ns = le.fit_transform(y_train_ns)\n",
        "y_test_ns = le.transform(y_test_ns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrvCQNleYGfZ"
      },
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': xgb.XGBClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCgDYOd7YJyJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# List to store accuracy scores for all models across four traits\n",
        "accuracy_scores = []\n",
        "\n",
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ8zDt4cbdZR"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_model(X_train_ns, X_test_ns, y_train_ns, y_test_ns, \"Intuition vs. Sensing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu7dOlUD1IA2"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSrxO5Bh1N8_"
      },
      "outputs": [],
      "source": [
        "for model in models.values():\n",
        "    model.probability = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLAAMrUslgK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_ns, y_train_ns)\n",
        "    y_score = model.predict_proba(X_test_ns)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test_ns, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "# Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve- N/S TF-IDF ')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jdxp7fvbRd"
      },
      "source": [
        "### ***EDA***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGOsszIn-CB"
      },
      "outputs": [],
      "source": [
        "df['full_preprocessed_text'] = df['posts'].apply(preprocess_text_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJiXjT20GvDN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the dataset for labels 'N' and 'S'\n",
        "df['label_NS'] = df['type'].apply(lambda x: 'N' if 'N' in x else ('S' if 'S' in x else np.nan))\n",
        "data_NS = df.dropna(subset=['label_NS'])\n",
        "\n",
        "# Text length analysis for labels 'N' and 'S'\n",
        "data_NS['post_length'] = data_NS['full_preprocessed_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plot the distribution of post length by type\n",
        "sns.histplot(data=data_NS, x='post_length', hue='label_NS', bins=50, kde=True)\n",
        "plt.title('Distribution of Post Length by Type (N vs S)')\n",
        "plt.xlabel('Post Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B-ZSTqOToo6I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "!pip install\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Spacy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Combine NLTK and sklearn stopwords\n",
        "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
        "\n",
        "# Filter posts related to 'N' and 'S' labels\n",
        "n_posts = data[data['type'].str.contains('N')]['posts']\n",
        "s_posts = data[data['type'].str.contains('S')]['posts']\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess the posts\n",
        "data['cleaned_posts'] = data['posts'].apply(preprocess_text)\n",
        "n_posts_cleaned = n_posts.apply(preprocess_text)\n",
        "s_posts_cleaned = s_posts.apply(preprocess_text)\n",
        "\n",
        "# Basic data inspection\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Text length analysis\n",
        "data['post_length'] = data['cleaned_posts'].apply(lambda x: len(x.split()))\n",
        "sns.histplot(data=data, x='post_length', hue='type', bins=50, kde=True)\n",
        "plt.title('Distribution of Post Length by Type')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZvGH6xx392s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")\n",
        "\n",
        "# Combine NLTK and sklearn stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess the posts\n",
        "data['cleaned_posts'] = data['posts'].apply(preprocess_text)\n",
        "\n",
        "# Filter posts related to 'N' and 'S' labels\n",
        "n_posts_cleaned = data[data['type'].str.contains('N')]['cleaned_posts']\n",
        "s_posts_cleaned = data[data['type'].str.contains('S')]['cleaned_posts']\n",
        "\n",
        "# Generate word clouds\n",
        "def create_wordcloud(text_data, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(text_data))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title, fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Create word clouds for 'N' and 'S' posts\n",
        "create_wordcloud(n_posts_cleaned, 'Word Cloud for Intuitive (N) Posts')\n",
        "create_wordcloud(s_posts_cleaned, 'Word Cloud for Sensing (S) Posts')\n"
      ],
      "metadata": {
        "id": "VVFTlLvt4K2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter posts related to 'N' and 'S' labels\n",
        "ns_posts_cleaned = data[data['type'].str.contains('N|S')]['cleaned_posts']\n",
        "\n",
        "# Combine all posts into a single string\n",
        "combined_text_ns = \" \".join(ns_posts_cleaned)\n",
        "\n",
        "# Generate and display the word cloud\n",
        "wordcloud_ns = WordCloud(width=800, height=400, background_color='white').generate(combined_text_ns)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_ns, interpolation='bilinear')\n",
        "plt.title('Combined Word Cloud for Intuitive (N) and Sensing (S) Posts', fontsize=18)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wy2Plsb94zkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***DL MODELS***"
      ],
      "metadata": {
        "id": "IEGjMkm55EAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1xpR9mWSJbn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from torchtext.vocab import GloVe\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "import tqdm\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier # This is where CatBoostClassifier is defined\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier # Remove CatBoostClassifier from here\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Gp8eLrPZSpte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
        "from torchtext.vocab import GloVe\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "SqxuzTToSkQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "585125e1"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2_ET-kdSJbs"
      },
      "outputs": [],
      "source": [
        "# Preprocess text (simple preprocessing considering only removal of URLs and lowercasing)\n",
        "data['posts'] = data['posts'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "data['posts'] = data['posts'].str.lower()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['posts'], data['class'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(X_ns, y_ns, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CZ8ovF0rCRq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9ROoV67SJbt"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_ns)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_ns)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_ns)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inh-EzdcSJbt"
      },
      "outputs": [],
      "source": [
        "# Train a Word2Vec model\n",
        "sentences = [sentence.split() for sentence in X_train_ns]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = w2v_model.wv[word] if word in w2v_model.wv else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiIH6bp5SJbu"
      },
      "outputs": [],
      "source": [
        "# Load the GloVe model\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Create an embedding matrix for Glove\n",
        "embedding_matrix_glove = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = glove[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_glove[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train_ns))\n",
        "print(type(X_test_ns))\n",
        "X_train_ns = X_train_ns.tolist()\n",
        "X_test_ns = X_test_ns.tolist()\n"
      ],
      "metadata": {
        "id": "YIbhrMH-cfwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr_sgKQdSJbu"
      },
      "outputs": [],
      "source": [
        "# Initialize Sentence Transformer Model\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encode sentences (for simplification we use mean pooling of embeddings)\n",
        "X_train_embeddings = sbert_model.encode(X_train_ns, show_progress_bar=True)\n",
        "X_test_embeddings = sbert_model.encode(X_test_ns, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orBqbuOxSJbv"
      },
      "outputs": [],
      "source": [
        "def build_model(embedding_matrix, lstm_type='lstm'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=100,\n",
        "                        weights=[embedding_matrix], trainable=False))\n",
        "    if lstm_type == 'lstm':\n",
        "        model.add(LSTM(100))\n",
        "    elif lstm_type == 'bilstm':\n",
        "        model.add(Bidirectional(LSTM(100)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data types of your target variable\n",
        "print(y_train_ns.dtype)\n",
        "\n",
        "# Check for string values in your features (after padding)\n",
        "print(np.unique(X_train_pad))\n",
        "\n",
        "# If you find string values, you need to convert them to numerical representations.\n",
        "# For example, if the padding token is a string, you can convert it to an integer:\n",
        "\n",
        "X_train_pad = np.where(X_train_pad == 'your_padding_token', 0, X_train_pad).astype(np.float32)\n",
        "X_test_pad = np.where(X_test_pad == 'your_padding_token', 0, X_test_pad).astype(np.float32)\n",
        "\n",
        "# If your target variable contains strings, you need to encode them numerically (e.g., using label encoding or one-hot encoding)"
      ],
      "metadata": {
        "id": "dKgCuqUlEllS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_p24a_USJbv"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "model_w2v_lstm = build_model(embedding_matrix, 'lstm')\n",
        "model_w2v_lstm.fit(X_train_pad, y_train_ns, epochs=35, batch_size=64, validation_data=(X_test_pad, y_test_ns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ro5m95hs9SRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for each class for the test data using the trained LSTM model\n",
        "y_pred_probs_lstm = model_w2v_lstm.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred_lstm = (y_pred_probs_lstm > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_lstm = classification_report(y_test, y_pred_lstm)\n",
        "print(\"Classification Report for LSTM Model:\")\n",
        "print(classification_report_lstm)\n"
      ],
      "metadata": {
        "id": "dd1osIXs495F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdUxqGhoIYdX"
      },
      "outputs": [],
      "source": [
        "# Bi-LSTM Model\n",
        "model_w2v_bilstm = build_model(embedding_matrix, 'bilstm')\n",
        "model_w2v_bilstm.fit(X_train_pad, y_train, epochs=35, batch_size=64, validation_data=(X_test_pad, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for each class for the test data using the trained Bi-LSTM model\n",
        "y_pred_probs_bilstm = model_w2v_bilstm.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred_bilstm = (y_pred_probs_bilstm > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_bilstm = classification_report(y_test, y_pred_bilstm)\n",
        "print(\"Classification Report for Bi-LSTM Model:\")\n",
        "print(classification_report_bilstm)\n"
      ],
      "metadata": {
        "id": "POMJduBT5I1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CNN Model with GloVe\n",
        "lstm_model_glove = build_lstm_model(embedding_matrix_glove)\n",
        "lstm_model_glove.fit(X_train_pad, y_train, epochs=35, batch_size=64, validation_data=(X_test_pad, y_test))\n"
      ],
      "metadata": {
        "id": "zzou5jbL-jZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict probabilities for each class for the test data using the trained  model\n",
        "y_pred_probs = lstm_model_glove.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_lstm = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report for CNN Model with GloVe:\")\n",
        "print(classification_report_lstm)\n"
      ],
      "metadata": {
        "id": "R0SW6Rpe_27T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm Model with Sentence Embeddings\n",
        "lstm_model_sentence = build_lstm_model(None)  # No embedding layer needed\n",
        "lstm_model_sentence.fit(X_train_embeddings, y_train, epochs=35, batch_size=64, validation_data=(X_test_embeddings, y_test))"
      ],
      "metadata": {
        "id": "OHg_htPV7v8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v01nf-IF7z67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***BERT***"
      ],
      "metadata": {
        "id": "Ss_rhRCVFZHK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTMhth_vFa8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "871bbda0"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40b0240f"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS, id2label=id2label, label2id=label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecae1b16"
      },
      "outputs": [],
      "source": [
        "SIZE= data.shape[0]\n",
        "\n",
        "train_texts= list(data.posts[:SIZE//2])\n",
        "\n",
        "val_texts=   list(data.posts[SIZE//2:(3*SIZE)//4 ])\n",
        "\n",
        "test_texts=  list(data.posts[(3*SIZE)//4:])\n",
        "\n",
        "train_labels= list(data.labels[:SIZE//2])\n",
        "\n",
        "val_labels=   list(data.labels[SIZE//2:(3*SIZE)//4])\n",
        "\n",
        "test_labels=  list(data.labels[(3*SIZE)//4:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "856348ff"
      },
      "outputs": [],
      "source": [
        "len(train_texts), len(val_texts), len(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bb03b5e"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e73e15a0"
      },
      "outputs": [],
      "source": [
        "class DataLoader(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for handling tokenized text data and corresponding labels.\n",
        "    Inherits from torch.utils.data.Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoader class with encodings and labels.\n",
        "\n",
        "        Args:\n",
        "            encodings (dict): A dictionary containing tokenized input text data\n",
        "                              (e.g., 'input_ids', 'token_type_ids', 'attention_mask').\n",
        "            labels (list): A list of integer labels for the input text data.\n",
        "        \"\"\"\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a dictionary containing tokenized data and the corresponding label for a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the data item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            item (dict): A dictionary containing the tokenized data and the corresponding label.\n",
        "        \"\"\"\n",
        "        # Retrieve tokenized data for the given index\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Add the label for the given index to the item dictionary\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of data items in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            (int): The number of data items in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c0932f6"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_encodings, train_labels)\n",
        "\n",
        "val_dataloader = DataLoader(val_encodings, val_labels)\n",
        "\n",
        "test_dataset = DataLoader(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4233af7a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "237770fa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
        "\n",
        "    Args:\n",
        "        pred (obj): An object containing label_ids and predictions attributes.\n",
        "            - label_ids (array-like): A 1D array of true class labels.\n",
        "            - predictions (array-like): A 2D array where each row represents\n",
        "              an observation, and each column represents the probability of\n",
        "              that observation belonging to a certain class.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the following metrics:\n",
        "            - Accuracy (float): The proportion of correctly classified instances.\n",
        "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
        "              and recall. Macro averaging calculates the metric independently for\n",
        "              each class and then takes the average.\n",
        "            - Precision (float): The macro precision, which is the number of true\n",
        "              positives divided by the sum of true positives and false positives.\n",
        "            - Recall (float): The macro recall, which is the number of true positives\n",
        "              divided by the sum of true positives and false negatives.\n",
        "    \"\"\"\n",
        "    # Extract true labels from the input object\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "\n",
        "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Return the computed metrics as a dictionary\n",
        "    return {\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c5b5a2f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir('../models_output/text_clf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5f54db2"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    # The output directory where the model predictions and checkpoints will be written\n",
        "    output_dir='../models_output/text_clf',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    #  The number of epochs, defaults to 3.0\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    # Number of steps used for a linear warmup\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy='steps',\n",
        "   # TensorBoard log directory\n",
        "    logging_dir='./multi-class-logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24db7d52"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    # the pre-trained model that will be fine-tuned\n",
        "    model=model,\n",
        "     # training arguments that we defined above\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader,\n",
        "    eval_dataset=val_dataloader,\n",
        "    compute_metrics= compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b98433d"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}